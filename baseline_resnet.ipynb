{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Caminho para a pasta de imagens\n",
    "images_path = '/teamspace/studios/this_studio/unsupervised_disaster/extracted_data/images'\n",
    "\n",
    "# Função para processar os nomes das imagens\n",
    "def process_image_metadata(images_folder):\n",
    "    data = []\n",
    "\n",
    "    # Iterar sobre todos os arquivos na pasta de imagens\n",
    "    for filename in os.listdir(images_folder):\n",
    "        if filename.endswith('.png') or filename.endswith('.jpg'):\n",
    "            # Separar informações do nome da imagem\n",
    "            parts = filename.split('_')\n",
    "            disaster_type = parts[0]  # Exemplo: 'guatemala-volcano' ou 'socal-fire'\n",
    "            disaster_status = \"Com desastre\" if \"post\" in parts[-2] else \"Sem desastre\"\n",
    "            image_reference = os.path.splitext(filename)[0]  # Nome sem extensão\n",
    "            \n",
    "            # Adicionar informações à lista\n",
    "            data.append({\n",
    "                \"Image Reference\": image_reference,\n",
    "                \"Image Name\": filename,\n",
    "                \"Disaster Type\": disaster_type,\n",
    "                \"Disaster Status\": disaster_status\n",
    "            })\n",
    "    \n",
    "    # Criar DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Processar imagens e criar DataFrame\n",
    "image_metadata_df = process_image_metadata(images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Reference</th>\n",
       "      <th>Image Name</th>\n",
       "      <th>Disaster Type</th>\n",
       "      <th>Disaster Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hurricane-harvey_00000015_pre_disaster</td>\n",
       "      <td>hurricane-harvey_00000015_pre_disaster.png</td>\n",
       "      <td>hurricane-harvey</td>\n",
       "      <td>Sem desastre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hurricane-harvey_00000228_pre_disaster</td>\n",
       "      <td>hurricane-harvey_00000228_pre_disaster.png</td>\n",
       "      <td>hurricane-harvey</td>\n",
       "      <td>Sem desastre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hurricane-michael_00000261_pre_disaster</td>\n",
       "      <td>hurricane-michael_00000261_pre_disaster.png</td>\n",
       "      <td>hurricane-michael</td>\n",
       "      <td>Sem desastre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hurricane-matthew_00000103_post_disaster</td>\n",
       "      <td>hurricane-matthew_00000103_post_disaster.png</td>\n",
       "      <td>hurricane-matthew</td>\n",
       "      <td>Com desastre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hurricane-harvey_00000216_pre_disaster</td>\n",
       "      <td>hurricane-harvey_00000216_pre_disaster.png</td>\n",
       "      <td>hurricane-harvey</td>\n",
       "      <td>Sem desastre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5593</th>\n",
       "      <td>guatemala-volcano_00000008_pre_disaster</td>\n",
       "      <td>guatemala-volcano_00000008_pre_disaster.png</td>\n",
       "      <td>guatemala-volcano</td>\n",
       "      <td>Sem desastre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5594</th>\n",
       "      <td>palu-tsunami_00000056_post_disaster</td>\n",
       "      <td>palu-tsunami_00000056_post_disaster.png</td>\n",
       "      <td>palu-tsunami</td>\n",
       "      <td>Com desastre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5595</th>\n",
       "      <td>socal-fire_00001156_post_disaster</td>\n",
       "      <td>socal-fire_00001156_post_disaster.png</td>\n",
       "      <td>socal-fire</td>\n",
       "      <td>Com desastre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5596</th>\n",
       "      <td>socal-fire_00000537_pre_disaster</td>\n",
       "      <td>socal-fire_00000537_pre_disaster.png</td>\n",
       "      <td>socal-fire</td>\n",
       "      <td>Sem desastre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5597</th>\n",
       "      <td>midwest-flooding_00000356_pre_disaster</td>\n",
       "      <td>midwest-flooding_00000356_pre_disaster.png</td>\n",
       "      <td>midwest-flooding</td>\n",
       "      <td>Sem desastre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5598 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Image Reference  \\\n",
       "0       hurricane-harvey_00000015_pre_disaster   \n",
       "1       hurricane-harvey_00000228_pre_disaster   \n",
       "2      hurricane-michael_00000261_pre_disaster   \n",
       "3     hurricane-matthew_00000103_post_disaster   \n",
       "4       hurricane-harvey_00000216_pre_disaster   \n",
       "...                                        ...   \n",
       "5593   guatemala-volcano_00000008_pre_disaster   \n",
       "5594       palu-tsunami_00000056_post_disaster   \n",
       "5595         socal-fire_00001156_post_disaster   \n",
       "5596          socal-fire_00000537_pre_disaster   \n",
       "5597    midwest-flooding_00000356_pre_disaster   \n",
       "\n",
       "                                        Image Name      Disaster Type  \\\n",
       "0       hurricane-harvey_00000015_pre_disaster.png   hurricane-harvey   \n",
       "1       hurricane-harvey_00000228_pre_disaster.png   hurricane-harvey   \n",
       "2      hurricane-michael_00000261_pre_disaster.png  hurricane-michael   \n",
       "3     hurricane-matthew_00000103_post_disaster.png  hurricane-matthew   \n",
       "4       hurricane-harvey_00000216_pre_disaster.png   hurricane-harvey   \n",
       "...                                            ...                ...   \n",
       "5593   guatemala-volcano_00000008_pre_disaster.png  guatemala-volcano   \n",
       "5594       palu-tsunami_00000056_post_disaster.png       palu-tsunami   \n",
       "5595         socal-fire_00001156_post_disaster.png         socal-fire   \n",
       "5596          socal-fire_00000537_pre_disaster.png         socal-fire   \n",
       "5597    midwest-flooding_00000356_pre_disaster.png   midwest-flooding   \n",
       "\n",
       "     Disaster Status  \n",
       "0       Sem desastre  \n",
       "1       Sem desastre  \n",
       "2       Sem desastre  \n",
       "3       Com desastre  \n",
       "4       Sem desastre  \n",
       "...              ...  \n",
       "5593    Sem desastre  \n",
       "5594    Com desastre  \n",
       "5595    Com desastre  \n",
       "5596    Sem desastre  \n",
       "5597    Sem desastre  \n",
       "\n",
       "[5598 rows x 4 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem por tipo de desastre natural:\n",
      "Disaster Type\n",
      "socal-fire             1646\n",
      "hurricane-michael       686\n",
      "hurricane-harvey        638\n",
      "hurricane-florence      638\n",
      "midwest-flooding        558\n",
      "hurricane-matthew       476\n",
      "santa-rosa-wildfire     452\n",
      "mexico-earthquake       242\n",
      "palu-tsunami            226\n",
      "guatemala-volcano        36\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Contagem total de imagens com desastre e sem desastre:\n",
      "Disaster Status\n",
      "Sem desastre    2799\n",
      "Com desastre    2799\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Contagem de imagens com desastre e sem desastre por tipo de desastre:\n",
      "Disaster Type        Disaster Status\n",
      "guatemala-volcano    Com desastre        18\n",
      "                     Sem desastre        18\n",
      "hurricane-florence   Com desastre       319\n",
      "                     Sem desastre       319\n",
      "hurricane-harvey     Com desastre       319\n",
      "                     Sem desastre       319\n",
      "hurricane-matthew    Com desastre       238\n",
      "                     Sem desastre       238\n",
      "hurricane-michael    Com desastre       343\n",
      "                     Sem desastre       343\n",
      "mexico-earthquake    Com desastre       121\n",
      "                     Sem desastre       121\n",
      "midwest-flooding     Com desastre       279\n",
      "                     Sem desastre       279\n",
      "palu-tsunami         Com desastre       113\n",
      "                     Sem desastre       113\n",
      "santa-rosa-wildfire  Com desastre       226\n",
      "                     Sem desastre       226\n",
      "socal-fire           Com desastre       823\n",
      "                     Sem desastre       823\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Contagem por tipo de desastre natural\n",
    "disaster_counts = image_metadata_df['Disaster Type'].value_counts()\n",
    "\n",
    "# Contagem de imagens com desastre e sem desastre (total e por tipo de desastre)\n",
    "status_counts_total = image_metadata_df['Disaster Status'].value_counts()\n",
    "status_counts_by_disaster = image_metadata_df.groupby(['Disaster Type', 'Disaster Status']).size()\n",
    "\n",
    "# Exibir os resultados\n",
    "print(\"Contagem por tipo de desastre natural:\")\n",
    "print(disaster_counts)\n",
    "\n",
    "print(\"\\nContagem total de imagens com desastre e sem desastre:\")\n",
    "print(status_counts_total)\n",
    "\n",
    "print(\"\\nContagem de imagens com desastre e sem desastre por tipo de desastre:\")\n",
    "print(status_counts_by_disaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações gerais\n",
    "IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Caminhos\n",
    "image_dir = '/Users/thiago/Desktop/dataset_AMNS/train/images'\n",
    "\n",
    "# 1. Preparação dos dados\n",
    "df = image_metadata_df.copy()\n",
    "df['Image Path'] = df['Image Name'].apply(lambda x: os.path.join(image_dir, x))\n",
    "df['Label'] = df['Disaster Status'].map({'Com desastre': 1, 'Sem desastre': 0})\n",
    "\n",
    "# Divisão em treino e validação\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['Label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe que o GPT criou para organizar o dataset\n",
    "class DisasterDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx]['Image Path']\n",
    "        label = self.dataframe.iloc[idx]['Label']\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformações \n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar datasets e dataloaders\n",
    "train_dataset = DisasterDataset(train_df, transform=data_transforms)\n",
    "val_dataset = DisasterDataset(val_df, transform=data_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        self.flatten_size = None  # Será calculado dinamicamente\n",
    "        self.fc1 = nn.Linear(1, 120)  # Placeholder\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "\n",
    "        if self.flatten_size is None:  # Calcular tamanho apenas uma vez\n",
    "            self.flatten_size = x.numel() // x.size(0)\n",
    "            self.fc1 = nn.Linear(self.flatten_size, 120).to(x.device)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual  # Soma o atalho à saída convolucional\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.layer1 = ResidualBlock(64, 64, stride=1)\n",
    "        self.layer2 = ResidualBlock(64, 128, stride=2)\n",
    "        self.layer3 = ResidualBlock(128, 128, stride=2)\n",
    "        self.layer4 = ResidualBlock(128, 256, stride=2)\n",
    "\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (layer1): ResidualBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (layer2): ResidualBlock(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): ResidualBlock(\n",
      "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): ResidualBlock(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "torch.backends.nnpack.enabled = False\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Instanciar o modelo\n",
    "model = ResNet(num_classes=1).to(device)\n",
    "#model = LeNet().to(device)\n",
    "model = model.float()\n",
    "print(model)\n",
    "\n",
    "# altere a função de custo e o otimizador\n",
    "#loss_fn = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device).unsqueeze(1)  # Ajustar dimensões do rótulo\n",
    "        X = X.to(torch.float32)\n",
    "        #print(f\"X dtype: {X.dtype}, X shape: {X.shape}\")\n",
    "        #print(f\"y dtype: {y.dtype}, y shape: {y.shape}\")\n",
    "    \n",
    "        # Forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        correct += ((torch.sigmoid(pred) > 0.5) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / len(dataloader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Função de validação\n",
    "def validate(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device).unsqueeze(1)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            total_loss += loss.item()\n",
    "            correct += ((torch.sigmoid(pred) > 0.5) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / len(dataloader.dataset)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 1/5 [08:01<32:06, 481.70s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.6298, Train Acc: 0.6447\n",
      "Val Loss: 0.6118, Val Acc: 0.6616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 2/5 [17:31<26:40, 533.61s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "Train Loss: 0.5715, Train Acc: 0.7057\n",
      "Val Loss: 0.5923, Val Acc: 0.6580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|██████    | 3/5 [25:40<17:06, 513.11s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "Train Loss: 0.5407, Train Acc: 0.7269\n",
      "Val Loss: 0.4883, Val Acc: 0.7589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|████████  | 4/5 [33:40<08:20, 500.23s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "Train Loss: 0.5028, Train Acc: 0.7544\n",
      "Val Loss: 0.6071, Val Acc: 0.7054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 5/5 [45:46<00:00, 549.21s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "Train Loss: 0.4923, Train Acc: 0.7660\n",
      "Val Loss: 0.4426, Val Acc: 0.7920\n",
      "Treinamento concluído!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm, trange\n",
    "\n",
    "train_loss_hist = []\n",
    "val_loss_hist = []\n",
    "train_acc_hist = []\n",
    "val_acc_hist = []\n",
    "\n",
    "for epoch in trange(EPOCHS, desc=\"Epochs\", unit=\"epoch\"):\n",
    "    train_loss, train_acc = train(train_loader, model, loss_fn, optimizer)\n",
    "    val_loss, val_acc = validate(val_loader, model, loss_fn)\n",
    "\n",
    "    train_loss_hist.append(train_loss)\n",
    "    val_loss_hist.append(val_loss)\n",
    "    train_acc_hist.append(train_acc)\n",
    "    val_acc_hist.append(val_acc)\n",
    "\n",
    "    tqdm.write(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    tqdm.write(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    tqdm.write(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"Treinamento concluído!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
